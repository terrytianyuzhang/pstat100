---
title: "PSTAT100 Lab6: Regression"
format: pdf
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message =  FALSE)
knitr::opts_chunk$set(warning =  FALSE)
knitr::opts_chunk$set(error =  FALSE)
bfcolor <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{\\textbf{%s}}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'><b>%s</b></span>", color, x)
  } else x
}

# Install necessary libraries if they aren't installed
if (!require(dplyr)) install.packages("dplyr")
if (!require(tidyr)) install.packages("tidyr")
if (!require(ggplot2)) install.packages("ggplot2")
```




# Introduction

This lab covers the nuts and bolts of fitting linear models. 
The linear model expresses a response variable, $y$, as a linear function of $p - 1$ explanatory variables $x_1, \dots, x_{p-1}$ and a random error $\epsilon$. 
Its general form is:

$$
y = \beta_0 + \beta_1 x_1 + \dots + \beta_{p-1}x_{p-1} + \epsilon, \quad \epsilon \sim N(0, \sigma^2)
$$

Usually, the response and explanatory variables and error term are indexed by observation $i = 1, \dots, n$, so that the model describes a dataset comprising $n$ values of each variable:

$$
y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_{p-1}x_{i,p-1} + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2)
$$

## Matrix Form Representation

$$
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
$$

where:

$$
\mathbf{y} =
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix},
\quad
\mathbf{X} =
\begin{bmatrix}
1 & x_{11} & \dots & x_{1,p-1} \\
1 & x_{21} & \dots & x_{2,p-1} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & \dots & x_{n,p-1}
\end{bmatrix},
\quad
\boldsymbol{\beta} =
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_{p-1}
\end{bmatrix},
\quad
\boldsymbol{\epsilon} =
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n
\end{bmatrix}
$$

**Fitting** a model of this form means **estimating the parameters** $\beta_0,...,\beta_{p-1}$ and $\sigma^2$ from a set of data.


## Estimation using Ordinary Least Squares (OLS)

-   The OLS estimate of $\boldsymbol{\beta}$ is given by:

$$
\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}' \mathbf{y}
$$

-   The error variance $\sigma^2$ is estimated as:

$$
\hat{\sigma}^2 = \frac{1}{n - p - 1} (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})' (\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}})
$$

When fitting a linear model, it is also of interest to quantify uncertainty by estimating the variability of $\widehat{\beta}$ and measure overall quality of fit. 
This lab illustrates that process and the computations involved.

## Objectives

In this lab, you'll learn how to:

-   compute OLS estimates;\
-   calculate fitted values and residuals;\
-   compute the error variance estimate;\
-   compute the variance-covariance matrix of $\widehat{\beta}$, which quantifies the variability of model estimates;\
-   compute standard errors for each model estimate;\
-   compute the proportion of variation captured by a linear model.

Throughout you'll use simple visualizations to help make the connection between fitted models and the aspects of a dataset that model features describe.




# Data: Fertility Rates

By way of data, you'll work with country indicators, total fertility rates, and gender indicators for a selection of countries in 2018, and explore the decline in fertility rates associated with developed nations. 

The data are stored in separate `.csv` files and imported below:

```{r}
# Load necessary library
library(dplyr)

# Read the data
fertility <- read.csv("data/fertility.csv")
country <- read.csv("data/country-indicators.csv")
gender <- read.csv("data/gender-data.csv")
```

The variables you'll work with in this portion are the following:

| Dataset   | Name                  | Variable                                      | Units                                  |
|-----------|-----------------------|-----------------------------------------------|-----------------------------------------|
| fertility | `fertility_total`       | National fertility rate                      | Average number of children per woman   |
| country   | `hdi`                   | Human development index                      | Index between 0 and 1 (0 is lowest, 1 is highest) |
| gender    | `educ_expected_yrs_f`    | Expected years of education for adult women | Years                                   |

Because the variables of interest are stored in three separate dataframes, you'll first need to **extract** them and **merge by country**.

```{r}
# Select variables of interest
fertility_sub <- fertility %>% select(Country, fertility_total)
gender_sub <- gender %>% select(Country, educ_expected_yrs_f)
country_sub <- country %>% select(Country, hdi)

# Merge datasets
reg_data <- fertility_sub %>%
  inner_join(gender_sub, by = "Country") %>%
  left_join(country_sub, by = "Country") %>%
  drop_na()

# Preview data
head(reg_data, 4)
```

We'll treat the fertility rates as our variable of interest.



# Exploratory analysis

A preliminary step in regression analysis is typically data exploration through scatterplots. 
The objective of exploratory analysis in this context is to identify **an approximately linear relationship** to model.

## Question 1: Education and fertility rate

Construct a **scatterplot** of total **fertility** against **expected years of education for women**. 
Label the axes 'Fertility rate' and 'Expected years of education for women'. 
Store this plot as `scatter_educ` and display the graphic.

`r bfcolor("YOUR ANSWER:", "red")`\

::: callout
```{r}
library(ggplot2)
# Scatterplot of fertility rate vs. expected years of education for women
# scatter_educ <- ggplot()
# scatter_educ
```
:::

This figure shows a clear **negative association** between fertility rate and women's educational attainment, and that the relationship is **roughly linear**. 
Next, check whether **HDI** seems to be related to fertility rate.

## Question 2: HDI and fertility rate

Now construct a **scatterplot** comparing **fertility rate** with **HDI**. 
Make sure you choose appropriate labels for your axes and plot. Store this plot as `scatter_hdi` and display the graphic.

`r bfcolor("YOUR ANSWER:", "red")`\

::: callout
```{r}
# Scatterplot of fertility rate vs. HDI
# scatter_hdi <- ggplot()
# scatter_hdi
```
:::

This figure shows **a negative relationship** between fertility rate and HDI; 
it may **not be exactly linear**, but a line should provide a decent approximation. 
So, the plots suggest that a **linear regression model** in one or both explanatory variables is reasonable.


# Simple linear regression

To start you'll fit a **simple linear model** regressing **fertility on education**.

First we'll need to store the quantities -- the response and explanatory variables -- needed for model fitting in the proper format. 
Recall that the linear model in matrix form is:

$$
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
$$

where:

$$
\mathbf{y} =
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix},
\quad
\mathbf{X} =
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n
\end{bmatrix},
\quad
\boldsymbol{\beta} =
\begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix},
\quad
\boldsymbol{\epsilon} =
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n
\end{bmatrix}
$$

Notice that the explanatory variable matrix **X** includes a column of ones for the intercept. So, the quantities needed are:

- **y**, a one-dimensional array of the total fertility rates for each country.
- **X**, a two-dimensional array with a column of ones (intercept) and a column of the expected years of education for women (explanatory variable).

The cell below constructs these arrays in R:

```{r}
# Retrieve response variable
y <- reg_data$fertility_total

# Construct explanatory variable (matrix)
x <- reg_data$educ_expected_yrs_f
x_with_leading1 <- model.matrix(~ x)

# Print first few rows of X
head(x_with_leading1)
```


## Estimation

Fitting a model refers to computing estimates; 
the `lm()` function in R will fit a linear regression model based on the response vector and explanatory variable matrix. The model structure follows:
$$
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
$$
The following code fits the simple linear model:
```{r}
# Fit simple linear model
lm_fit <- lm(y ~ x, data = reg_data)

# Display summary of results
summary(lm_fit)
```

## Extracting Estimates

-   The **coefficient estimates** $\widehat{\beta}_0$ and $\widehat{\beta}_1$ are obtained using:
```{r}
# Coefficients
coef(lm_fit)
```

-   The **error variance estimate** $\widehat{\sigma}^2$  can be retrieved as: 
```{r}
# Variance estimate
sigma_hat2 <- summary(lm_fit)$sigma^2
sigma_hat2
```

-   The **variance-covariance matrix** of the **estimated coefficients** is:
$$
\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}
$$
which can be retrieved in R using:
```{r}
# Variance-covariance matrix of coefficients
vcov(lm_fit)
```

## Model Interpretation
A standard metric often reported with linear models is the **$R^2$ score**, 
which quantifies the **proportion of variation in the response explained by the model**:
```{r}
# Compute R-squared
summary(lm_fit)$r.squared
```
So, the expected years of education for women in a country explains $72.38\%$ of variability in fertility rates, and furthermore, according to the fitted model:

-   For a country in which women are **entirely uneducated**, the estimated mean fertility rate is $7.5$ children on average by the end of a woman's reproductive period.

-   Each additional year of education for women is associated with a **decrease** in a country's fertility rate by an estimated 0.43.

-   After accounting for women's education levels, fertility rates vary by a standard deviation of $0.66=\sqrt{0.438}$ across countries.

-   This model provides an initial assessment of the relationship, but further **diagnostics** are necessary to validate assumptions.


## Question 3: center the explanatory variable

Note that no countries report an expected zero years of education for women, so the meaning of the intercept is artificial. 
As we saw in lecture, **centering** the explanatory variable can improve **interpretability** of the intercept. 
**Center** the expected years of education for women and **refit** the model by following the steps outlined below. 
Display the **coefficient estimates** and **standard errors**.

`r bfcolor("YOUR ANSWER:", "red")`\

::: callout
```{r}
# Center the education column by subtracting its mean from each value
# educ_ctr <- 

# Fit new model
# lm_ctr <- 

# Extract results
# summary(lm_ctr)

# Arrange estimates and standard errors in a dataframe and display
# coef_tbl <- data.frame()

# print(coef_tbl)
```
:::


# Fitted values and residuals

## Fitted values

The **fitted value** for $y_i$ is the value along the line specified by the model that corresponds to the matching explanatory variable $x_i$. In other words:
$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
$$
These can be obtained directly from the fitted model in R:

```{r}
# Fitted values
fitted_values <- fitted(lm_fit)

# Display first few fitted values
head(fitted_values)
```

The result is an array with **length** matching the number of observations $\boldsymbol{X}$ used to fit the model. 
The fitted values correspond to the **predicted response** for each explanatory variable.

## Residuals
Recall that **model residuals** are the **difference** between observed and fitted values:
$$
e_i = y_i - \hat{y}_i 
$$
Residuals can be retrieved similarly as an attribute of the regression results:
```{r}
# Obtain residuals
residuals <- residuals(lm_fit)

# Display first few residuals
head(residuals)
```
Again, `residuals` is an array with **length** matching the number of observations $\boldsymbol{X}$ used to fit the model. 
And these residuals are returned in the same order as the original observations.


## Question 4: calculations 'by hand'

Calculate the **fitted values** and **residuals** *manually*. 
Store the results as arrays `fitted_manual` and `resid_manual`, respectively.

*Hint:* Use **matrix-vector multiplication**.

`r bfcolor("YOUR ANSWER:", "red")`\

::: callout
```{r}
X <- x_with_leading1

# Compute fitted values manually
# fitted_manual <- 

# Compute residuals manually
# resid_manual <- 

# Display first few values
# head(fitted_manual)
# head(resid_manual)
```
:::

It is often convenient to add the **fitted values** and **residuals** as new columns in `reg_data`.
```{r}
# Append fitted values and residuals
reg_data$fitted_slr <- fitted(lm_fit)
reg_data$resid_slr <- residuals(lm_fit)

# Display first few rows
head(reg_data, 3)
```

## Visualizing the Model
We can use this augmented dataframe to visualize the deterministic part of the model:
```{r}
# Construct scatterplot with fitted line
ggplot(reg_data, aes(x = educ_expected_yrs_f, y = fertility_total)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Fertility Rate vs. Education",
       x = "Expected Years of Education for Women",
       y = "Fertility Rate")

```

### Uncertainty Bands
To obtain **uncertainty bands** about the **estimated mean**, we'll compute predictions at each observed value using confidence intervals.
```{r}
# Compute confidence intervals for estimated mean
conf_int <- predict(lm_fit, interval = "confidence")

# Append lower and upper bounds to the data
reg_data$lwr_mean <- conf_int[, "lwr"]
reg_data$upr_mean <- conf_int[, "upr"]

# Display first few rows
head(reg_data)
```
Now, we can visualize the uncertainty bands:
```{r}
# Construct plot with uncertainty bands
ggplot(reg_data, aes(x = educ_expected_yrs_f, y = fertility_total)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  geom_ribbon(aes(ymin = lwr_mean, ymax = upr_mean), alpha = 0.2) +
  labs(title = "Fertility Rate vs. Education with Confidence Bands",
       x = "Expected Years of Education for Women",
       y = "Fertility Rate")
```

As discussed in lecture, we can also compute and display uncertainty bounds for **predicted observations** (rather than the mean). 
```{r}
head(predict(lm_fit, interval = "prediction"))
```
These will be wider, because there is more uncertainty associated with predicting observations compared with estimating the mean.


## Question 5: Prediction Intervals

The **standard error** for **predictions** is stored with the output of `predict()` as part of the confidence interval calculation. 
The prediction standard error captures **variability** when predicting new observations rather than estimating the mean.

Use this method to compute **95% uncertainty bounds for the predicted observations**. 
Add the lower and upper bounds as new columns in `reg_data`, named `lwr_obs` and `upr_obs`, respectively. 
Construct **a plot** showing data scatter, the **model predictions**, and **prediction uncertainty bands**.

`r bfcolor("YOUR ANSWER:", "red")`\

::: callout
```{r}
# Compute prediction intervals
# pred_int <- 

# Store lower and upper bounds in the dataset
# reg_data$lwr_obs <- 
# reg_data$upr_obs <- 

# Display first few rows
# head(reg_data)
```
:::

**Visualization of Prediction Intervals:**

Now, we can create a plot displaying both confidence intervals (for the mean) and prediction intervals (for new observations):

`r bfcolor("YOUR ANSWER:", "red")`\

::: callout
```{r}
# Compute confidence intervals for estimated mean
conf_int <- predict(lm_fit, interval = "confidence")

# Append lower and upper bounds to the data
reg_data$lwr_mean <- conf_int[, "lwr"]
reg_data$upr_mean <- conf_int[, "upr"]

# Compute prediction intervals
pred_int <- predict(lm_fit, interval = "prediction")

# Store lower and upper bounds in the dataset
reg_data$lwr_obs <- pred_int[, "lwr"]
reg_data$upr_obs <- pred_int[, "upr"]

# Construct plot showing prediction uncertainty
ggplot(reg_data, aes(x = educ_expected_yrs_f, y = fertility_total)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue") +
  geom_ribbon(aes(ymin = lwr_mean, ymax = upr_mean), 
              fill = "blue", alpha = 0.2) +  # Confidence interval
  geom_ribbon(aes(ymin = lwr_obs, ymax = upr_obs), 
              fill = "red", alpha = 0.2) +    # Prediction interval
  labs(title = "Fertility Rate vs. Education with Confidence and Prediction Intervals",
       x = "Expected Years of Education for Women",
       y = "Fertility Rate")
```
:::

### Interpretation
-   The **confidence interval** (shaded in blue) represents uncertainty in estimating the mean response.

-   The **prediction interval** (shaded in red) is *wider* because it accounts for additional variability when predicting new observations.

-   The **prediction band** is interpreted as follows: 95% of the time, the true observed value will fall within this range.


## Question 6: coverage

What proportion of observed values are within the prediction bands? Compute and store this value as `coverage_prop`.

`r bfcolor("YOUR ANSWER:", "red")`\

::: callout
```{r}
# Compute the proportion of observed values within prediction bands
# coverage_prop <- 
  
# Display the computed proportion
# coverage_prop
```
:::


# Multiple Linear Regression

Now let's consider adding the **human development factor** to the model. 
First, let's investigate the *univariate* relationship between **HDI** (Human Development Index) and **fertility rate**.

A scatterplot is shown below with a regression line overlaid. The relationship may not be perfectly linear, but a line should provide a decent approximation.

```{r}
# Scatterplot of HDI vs Fertility Rate with Regression Line
ggplot(reg_data, aes(x = hdi, y = fertility_total)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(title = "Fertility Rate vs. Human Development Index",
       x = "Human Development Index (HDI)",
       y = "Fertility Rate")
```

## Question 7: Fit a Model with HDI Only

Fit the model plotted above. Display the **coefficient estimates**, **standard errors**, and **$R^2$ statistic**.

`r bfcolor("YOUR ANSWER:", "red")`\

::: callout
```{r}
# Fit simple linear regression with HDI only
# lm_hdi <- 

# Display summary of results
# summary(lm_hdi)

# Coefficients
# paste("Coefficient estimates (beta0): ", )
# paste("Coefficient estimates (beta1): ", )

# Variance estimate
# paste("Error variance estimate is: ",  )

# Variance-covariance matrix
# vcov <- 
# paste("Standard errors of estimated beta0 are: ", )
# paste("Standard errors of estimated beta1 are: ", )

# Compute R-squared
# paste("R^2 statistic is: ", )
```
:::

You should have observed that this model also explains about **70% of variance in fertility rates**. 
This suggests that **HDI** is an **equally good predictor of fertility rates**.

However, HDI is **highly correlated** with women's education. 
Let's compute their **correlation**:

```{r}
# Compute correlation between HDI and education
cor(reg_data$hdi, reg_data$educ_expected_yrs_f)
```
So what do you think will happen if we fit a model with both explanatory variables? 

-   Will fertility rate have a stronger association with one or the other? 

-   Will the coefficient estimates also be highly correlated? 

Take a moment to consider this and come up with a hypothesis.


## Multiple Linear Regression: HDI and Education

The model is fit **exactly** the same way as the **SLR models**â€”the only difference is that instead of using a **single predictor**, we now use **two predictors (HDI and Education).**

```{r}
# Construct explanatory variable matrix with both predictors
mlr_fit <- lm(fertility_total ~ hdi + educ_expected_yrs_f, data = reg_data)

# Store results
summary(mlr_fit)
```

### Extracting Estimates

```{r}
# Coefficients
coef(mlr_fit)

# Standard errors
sqrt(diag(vcov(mlr_fit)))

# Variance estimate
sigma_hat2_mlr <- summary(mlr_fit)$sigma^2
sigma_hat2_mlr
```

### Coefficient Interpretation

-   The association with HDI is **weaker in the multiple linear model** (around -4.13) compared to the simple linear model (-7.00 when education is not included).

-   Similarly, the association with education is **also weaker** (around -0.20) compared to the simple model (-0.43 when HDI is not included).

This is due to **multicollinearity**, where HDI and education are **highly correlated**. Let's recall the correlation between them:
```{r}
# Compute correlation between HDI and education
cor(reg_data$hdi, reg_data$educ_expected_yrs_f)
```

### Assessing Multicollinearity
```{r}
# Compute variance-covariance matrix
vcov_mlr <- vcov(mlr_fit)

# Compute correlation between coefficient estimates
stderr_mlr <- sqrt(diag(vcov_mlr))
corr_mx <- diag(1/stderr_mlr) %*% vcov_mlr %*% diag(1/stderr_mlr)

# Display correlation between coefficient estimates
corr_mx[1,2]  # Correlation between HDI and Education coefficient estimates
```

### Model Fit and $R^2$ Statistic
The multiple linear regression model captures a little bit more variance than either simple linear regression model individually:
```{r}
# Compute R-squared
summary(mlr_fit)$r.squared
```

### Discussion

-   The MLR model doesn't add much value in terms of fit, so if that is our only concern we might prefer one of the SLR models. 

-   However, the presence of additional predictors changes the parameter interpretation -- in the MLR model, the coefficients give the estimated changes in mean fertility rate associated with changes in each explanatory variable after accounting for the other explanatory variable. This is one way of understanding why the estimates change so much in the presence of additional explanatory variables -- the association between, e.g., HDI and fertility, is different than the association between HDI and fertility after adjusting for women's expected education.

-   More broadly, these data are definitely not a representative sample of any particular population of nations -- the countries (observational units) are conveniently chosen based on which countries reported data. So there is no scope of inference here, for any of the models we've fit.

-   Although we can't claim that, for example, 'the mean fertility rate decreases with education at a rate of 0.2 children per woman per expected year of education after accounting for development status', we can say 'among the countries reporting data, the mean fertility rate decreases with education at a rate of 0.2 children per woman per expected year of education after accounting for development status'. This is a nice example of how a model might be used in a descriptive capacity.



# Bootstrap for Estimating Sampling Distribution

The bootstrap method is a **resampling** technique that allows us 
to estimate the **sampling distribution of a statistic** (such as the **mean**) 
*without relying on theoretical assumptions*. 
It is especially useful when the underlying distribution of the data is unknown or difficult to model analytically.

## Bootstrap procedure

The **bootstrap procedure** follows these steps:

1.    **Resample with replacement** from the observed data, creating a new sample of the same size.

2.    **Compute the statistic of interest** (e.g., sample mean) for each resampled dataset.

3.    **Repeat the process** many times (e.g., 1000 iterations) to generate an empirical distribution of the statistic.

4.    **Analyze the results**, including estimating confidence intervals.

## Bootstrap Sampling of the Mean Fertility Rate
We will apply the bootstrap method to estimate the **sampling distribution of the mean fertility rate**.

### Step 1: Bootstrap Resampling
We generate **1000 bootstrap samples**, each obtained by randomly resampling (with replacement) from the original dataset.
```{r}
# Load necessary libraries
library(tibble)
library(rsample)
library(ggplot2)
library(purrr)

# Set seed for reproducibility
set.seed(123)

# Create a tibble with fertility rate
bootstrap_data <- tibble(fertility = reg_data$fertility_total) |> 
  bootstraps(times = 1000) |> 
  mutate(bootstrap_mean = map_dbl(splits, ~ mean(as_tibble(.)$fertility)))

# Display first few bootstrap sample means
head(bootstrap_data$bootstrap_mean)
```

### Step 2: Visualizing the Bootstrap Distribution

A **histogram** of the **bootstrap sample means** allows us to approximate the sampling distribution.
```{r}
# Plot the bootstrap distribution of sample means
bootstrap_data |> 
  ggplot() + 
  geom_histogram(aes(x = bootstrap_mean), bins = 30, 
                 fill = "blue", alpha = 0.6) + 
  geom_vline(aes(xintercept = mean(reg_data$fertility_total)), 
             col = "red", linetype = "dashed") +
  labs(title = "Bootstrap Distribution of Sample Mean",
       x = "Bootstrap Sample Mean",
       y = "Frequency") 
```


### Interpretation:

-   The histogram represents the **empirical distribution** of the **sample mean**.

-   The red dashed line represents the **original sample mean**.

-   The bootstrap method provides an **approximation of the sampling distribution**, helping us quantify uncertainty in the sample mean.


## Bootstrap Confidence Intervals
A key application of bootstrap methods is **constructing confidence intervals** for an estimator. 
We can estimate a 95% confidence interval for **the mean fertility rate** using the **percentile method**.

### Step 3: Computing the 95% Confidence Interval

```{r}
# Compute 95% confidence interval from bootstrap distribution
ci_boot <- quantile(bootstrap_data$bootstrap_mean, probs = c(0.025, 0.975))
ci_boot
```

### Interpretation:

-   The confidence interval provides a **plausible** range for the **population mean**.

-   Unlike theoretical methods, bootstrap confidence intervals **do not require normality** assumptions.


## Question 8: Bootstrap Sampling of the Median Fertility Rate

Instead of the mean, 

-   Estimate the **sampling distribution** of the **median fertility rate** using **1000 bootstrap resamples**.

-   Visualize the bootstrap distribution using a **histogram**.

-   **Compare** the bootstrap sample medians with the original sample median.

`r bfcolor("YOUR ANSWER:", "red")`\

::: callout
```{r}
# Set seed for reproducibility
set.seed(123)

# Create bootstrap resamples and compute median for each
# bootstrap_data_median <- ...

# Plot the bootstrap distribution of sample medians
#bootstrap_data_median |> 
#  ggplot() + ...
```
:::


## Question 9: Bootstrap Confidence Interval for HDI Mean

Now, compute a **95% confidence interval** for the **median fertility rate** using the **1000 bootstrap samples** we have drawn.


`r bfcolor("YOUR ANSWER:", "red")`\

::: callout
```{r}
# Compute 95% confidence interval for the median
# ci_boot_median <- ...
# ci_boot_median
```
:::

## Summary
-   Bootstrap resampling allows us to estimate the **sampling distribution of a statistic**.

-   The bootstrap confidence interval provides an **empirical way** to quantify estimation uncertainty.

-   This method is particularly useful when **theoretical assumptions** about the data, e.g., properties of the underlying distribution, are **uncertain**.



# Submission

1.  Rename and save the notebook.\
2.  Restart the kernel and run all cells. (**CAUTION**: if your notebook is not saved, you will lose your work.)\
3.  Carefully look through your notebook and verify that all computations execute correctly. You should see **no errors**; if there are any errors, make sure to correct them before you submit the notebook.\
4.  Download the notebook as an `.qmd` file. This is your backup copy.\
5.  Export the notebook as PDF and upload to Canvas.




